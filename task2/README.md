# Task 2: PreparaciÃ³n de Datos - DetecciÃ³n de Phishing

## ğŸš€ CÃ³mo ejecutar

```bash
cd task2
python main.py
```

## ğŸ“Š QuÃ© hace

1. **Carga** el dataset y elimina columna 'url'
2. **Codifica** status: legitimate=0, phishing=1
3. **Selecciona** las 2 features con mayor correlaciÃ³n (`google_index`, `page_rank`)
4. **Escala** datos usando StandardScaler (implementado desde cero)
5. **Divide** en train (80%) y test (20%) con estratificaciÃ³n

## ğŸ“ˆ Salida

- **Datos procesados**: 87 features, 9,144 train, 2,286 test
- **GrÃ¡ficos generados**:
  - `correlaciones_features.png` - Top 15 features
  - `top_features_scatter.png` - Scatter plots de mejores features
  - `feature_space_2d.png` - Espacio 2D para fronteras de decisiÃ³n

## ğŸ“¦ Estructura modular

- `data_loading.py` - Limpieza y codificaciÃ³n
- `feature_selection.py` - SelecciÃ³n por correlaciÃ³n
- `data_scaling.py` - StandardScaler manual
- `data_splitting.py` - Train/test split manual
- `main.py` - Pipeline completo

## âœ… Sin sklearn

Todo implementado desde cero con **pandas/numpy** para entender cÃ³mo funciona internamente.
